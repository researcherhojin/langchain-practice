import streamlit as st
import logging
from pathlib import Path
import sys
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_models import ChatOpenAI
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain_core.runnables import RunnablePassthrough
from langchain.schema import Document
import tempfile
import os
import chardet

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout), logging.FileHandler("app.log")],
)
logger = logging.getLogger(__name__)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Literary Analysis RAG", page_icon="ğŸ“š", layout="wide")

# ì´ˆê¸° ì„¸ì…˜ ìƒíƒœ ì„¤ì •
if "memory" not in st.session_state:
    st.session_state["memory"] = ConversationBufferMemory(
        return_messages=True, memory_key="history"
    )
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "chain" not in st.session_state:
    st.session_state["chain"] = None


# OpenAI API í‚¤ ê²€ì¦ í•¨ìˆ˜
def is_valid_openai_api_key(api_key):
    if not api_key or not isinstance(api_key, str):
        return False
    if not api_key.startswith("sk-") or len(api_key) < 20:
        return False
    return True


# íŒŒì¼ ì¸ì½”ë”© ê°ì§€ í•¨ìˆ˜
def detect_file_encoding(file_content):
    result = chardet.detect(file_content)
    logger.info(f"Detected encoding: {result}")
    encoding = result["encoding"]
    # ê°ì§€ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ utf-8-sigë¡œ ì„¤ì •
    if encoding is None:
        encoding = "utf-8-sig"
    return encoding


# ì²´ì¸ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜
def get_chain_history(x):
    if "memory" in st.session_state:
        try:
            return st.session_state["memory"].load_memory_variables({})["history"]
        except Exception as e:
            logger.error(f"Error loading memory: {str(e)}")
            return []
    return []


# ë‹¤ì–‘í•œ ì¸ì½”ë”©ì„ ì‹œë„í•˜ì—¬ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
def load_documents_with_multiple_encodings(file_path, primary_encoding):
    """ì£¼ì–´ì§„ íŒŒì¼ì„ ì—¬ëŸ¬ ì¸ì½”ë”©ì„ ì‹œë„í•˜ì—¬ ë¬¸ìì—´ë¡œ ë¡œë“œí•œ í›„ Document ê°ì²´ë¡œ ë°˜í™˜"""
    encodings_to_try = [primary_encoding, "cp949", "utf-8-sig", "latin-1"]
    with open(file_path, "rb") as f:
        raw_data = f.read()

    for enc in encodings_to_try:
        try:
            text = raw_data.decode(enc)
            return [Document(page_content=text)]
        except UnicodeDecodeError:
            logger.warning(f"Failed to decode with {enc}, trying next...")

    # ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨
    raise RuntimeError(f"Unable to load document with fallback encodings.")


# RAG íŒŒì´í”„ë¼ì¸ ì„¤ì • í•¨ìˆ˜
def setup_rag_pipeline(file_path, openai_api_key, encoding):
    try:
        logger.info(f"Attempting to load file from path: {file_path}")
        logger.info(f"File exists: {Path(file_path).exists()}")
        logger.info(f"File size: {Path(file_path).stat().st_size} bytes")
        logger.info(f"Using encoding: {encoding}")

        # ë¬¸ì„œ ë¡œë“œ (ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„)
        documents = load_documents_with_multiple_encodings(file_path, encoding)
        logger.info("Successfully loaded document")

        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = CharacterTextSplitter(
            chunk_size=2000, chunk_overlap=200, separator="\n"
        )
        splits = text_splitter.split_documents(documents)
        logger.info(f"Split documents into {len(splits)} chunks")

        # ì„ë² ë”© ì„¤ì •
        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vectorstore = FAISS.from_documents(splits, embeddings)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "ì•„ë˜ì˜ contextë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\nContext: {context}",
                ),
                MessagesPlaceholder(variable_name="history"),
                ("human", "{question}"),
            ]
        )

        # LLM ì„¤ì •
        model = ChatOpenAI(
            model="gpt-4o-mini-2024-07-18",
            temperature=0.1,
            openai_api_key=openai_api_key,
        )

        # ì²´ì¸ êµ¬ì„±
        chain = (
            {
                "context": retriever,
                "question": RunnablePassthrough(),
                "history": get_chain_history,
            }
            | prompt
            | model
        )

        return chain

    except Exception as e:
        logger.error(f"Error in setup_rag_pipeline: {str(e)}", exc_info=True)
        raise


# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.title("ğŸ“š Literary Analysis RAG")

    # OpenAI API í‚¤ ì…ë ¥
    openai_api_key = st.text_input(
        "OpenAI API Key",
        type="password",
        placeholder="sk-...",
        help="Get your API key from https://platform.openai.com/account/api-keys",
    )

    # API í‚¤ ìƒíƒœ í‘œì‹œ
    if openai_api_key:
        if is_valid_openai_api_key(openai_api_key):
            st.success("API key provided", icon="âœ…")
        else:
            st.error("Invalid API key format. It should start with 'sk-'")
            openai_api_key = None
    else:
        st.warning("Please enter your OpenAI API key", icon="âš ï¸")

    # ê¹ƒí—ˆë¸Œ ë§í¬
    st.markdown(
        """
       ### Links
       - [GitHub Repository](https://github.com/researcherhojin/langchain-practice)
       """
    )

    st.markdown("---")

# ë©”ì¸ í˜ì´ì§€ ì„¤ì •
st.title("ğŸ“š Literary Analysis RAG")
st.markdown(
    """
   Upload a text file and ask questions about it. 
   The app will use RAG (Retrieval Augmented Generation) to provide accurate answers.
   """
)

# íŒŒì¼ ì—…ë¡œë”
uploaded_file = st.file_uploader(
    "Upload a text file", type=["txt"], help="Upload a text file to analyze"
)

# íŒŒì¼ì´ ì—…ë¡œë“œë˜ê³  API í‚¤ê°€ ì œê³µëœ ê²½ìš° ì²˜ë¦¬
if uploaded_file and openai_api_key and is_valid_openai_api_key(openai_api_key):
    try:
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        file_content = uploaded_file.read()
        logger.info(f"Read {len(file_content)} bytes from uploaded file")

        # íŒŒì¼ ì¸ì½”ë”© ê°ì§€
        detected_encoding = detect_file_encoding(file_content)
        logger.info(f"Detected file encoding: {detected_encoding}")

        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(
            delete=False, suffix=".txt", mode="wb"
        ) as tmp_file:
            tmp_file.write(file_content)
            tmp_file_path = tmp_file.name
            logger.info(f"Created temporary file at: {tmp_file_path}")

        try:
            # RAG íŒŒì´í”„ë¼ì¸ ì„¤ì •
            st.session_state["chain"] = setup_rag_pipeline(
                tmp_file_path, openai_api_key, detected_encoding
            )

            # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
            for message in st.session_state["messages"]:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

            # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
            if prompt := st.chat_input("Ask a question about the text"):
                st.session_state["messages"].append({"role": "user", "content": prompt})
                with st.chat_message("user"):
                    st.markdown(prompt)

                with st.chat_message("assistant"):
                    response = st.session_state["chain"].invoke(prompt)
                    st.markdown(response.content)

                    # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
                    st.session_state["messages"].append(
                        {"role": "assistant", "content": response.content}
                    )
                    st.session_state["memory"].save_context(
                        {"input": prompt}, {"output": response.content}
                    )

        except Exception as e:
            logger.error(f"Error during pipeline execution: {str(e)}", exc_info=True)
            st.error(f"An error occurred during processing: {str(e)}")

        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            try:
                os.unlink(tmp_file_path)
                logger.info(f"Successfully removed temporary file: {tmp_file_path}")
            except Exception as e:
                logger.error(f"Error removing temporary file: {str(e)}", exc_info=True)

    except Exception as e:
        logger.error(f"Error in file upload handling: {str(e)}", exc_info=True)
        st.error(f"An error occurred while handling the uploaded file: {str(e)}")

else:
    if not uploaded_file:
        st.info("Please upload a text file to start.")
    if not openai_api_key or not is_valid_openai_api_key(openai_api_key):
        st.info("Please provide a valid OpenAI API key.")

# ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ë²„íŠ¼
if st.sidebar.button("Clear Chat History"):
    st.session_state["messages"] = []
    st.session_state["memory"].clear()
    st.session_state["chain"] = None
    st.experimental_rerun()
